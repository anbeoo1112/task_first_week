import os
import nest_asyncio
from typing import Dict, Optional
import traceback

from extract_thinker import (
    Process, Extractor, ImageSplitter, TextSplitter, SplittingStrategy, CompletionStrategy, LLM
)
from core.config import config
from core.classifications import getClassificationsList
from core.utils import (
    countPages, findCategory, sanitizePageGroups, 
    makeSuccessResponse, makeErrorResponse
)

nest_asyncio.apply()

class DocumentProcessor:
    # Class x·ª≠ l√Ω t√†i li·ªáu ch√≠nh c·ªßa h·ªá th·ªëng.
    def __init__(self, model: Optional[str] = None, 
                 strategy: CompletionStrategy = CompletionStrategy.CONCATENATE):
        config.validate()
        self._model = model or config.processing.model
        self._strategy = strategy
    
    def run(self, filePath: str) -> Dict:
        # H√†m ch√≠nh ƒë·ªÉ x·ª≠ l√Ω t√†i li·ªáu v√† tr√≠ch xu·∫•t th√¥ng tin.
        
        # Args:
        #     filePath (str): ƒê∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi ƒë·∫øn file c·∫ßn x·ª≠ l√Ω.
            
        # Returns:
        #     Dict: K·∫øt qu·∫£ tr√≠ch xu·∫•t ho·∫∑c th√¥ng b√°o l·ªói.
        if not os.path.exists(filePath):
            return makeErrorResponse("File kh√¥ng t·ªìn t·∫°i")
        
        try:
            # 1. Kh·ªüi t·∫°o Loader v√† ƒë·∫øm s·ªë trang
            loader, vision, loaderName = config.createLoader(filePath)
            pageCount = countPages(filePath)
            
            print(f"üîÑ ƒêang x·ª≠ l√Ω: {loaderName}, vision={vision}, s·ªë trang={pageCount}")
            
            # 2. C·∫•u h√¨nh Extractor
            extractor = Extractor()
            extractor.load_document_loader(loader)
            extractor.load_llm(LLM(self._model))
            
            # 3. Ch·ªçn chi·∫øn l∆∞·ª£c x·ª≠ l√Ω d·ª±a tr√™n s·ªë trang
            if pageCount == 1:
                return self.extractSinglePage(extractor, filePath, vision, loaderName)
            
            return self.extractMultiPage(filePath, vision, loaderName, pageCount)
                
        except Exception as e:
            traceback.print_exc()
            return makeErrorResponse(str(e)[:200])
    
    def extractSinglePage(self, extractor: Extractor, filePath: str, vision: bool, loaderName: str) -> Dict:
        # X·ª≠ l√Ω t√†i li·ªáu ƒë∆°n trang (Single Page).
        # Ph√¢n lo·∫°i t√†i li·ªáu
        classifications = getClassificationsList()
        result = extractor.classify(filePath, classifications, vision=vision)
        
        if not result or result.name == "Other":
            return makeSuccessResponse(category="Other", loader=loaderName, vision=vision)
        
        contract = result.classification.contract if result.classification else None
        data = None
        
        # Tr√≠ch xu·∫•t d·ªØ li·ªáu n·∫øu t√¨m th·∫•y Contract ph√π h·ª£p
        if contract:
            extractedObj = extractor.extract(filePath, contract, vision=vision, completion_strategy=self._strategy)
            data = extractedObj.model_dump() if hasattr(extractedObj, 'model_dump') else extractedObj
        
        return makeSuccessResponse(
            category=findCategory(result.name) or result.name,
            docType=result.name,
            data=data,
            confidence=getattr(result, 'confidence', None),
            loader=loaderName,
            vision=vision
        )
    
    def extractMultiPage(self, filePath: str, vision: bool, loaderName: str, pageCount: int) -> Dict:
        # X·ª≠ l√Ω t√†i li·ªáu ƒëa trang (Multi Page).
        # Bao g·ªìm c√°c b∆∞·ªõc: Split (T√°ch trang) -> Sanitize (S·ª≠a l·ªói trang) -> Extract (Tr√≠ch xu·∫•t).
        print("üìÑ Ph√°t hi·ªán t√†i li·ªáu nhi·ªÅu trang. ƒêang ti·∫øn h√†nh t√°ch (Splitting)...")
        
        # 1. Chu·∫©n b·ªã Loader ri√™ng bi·ªát cho b∆∞·ªõc Split
        splitLoader, _, _ = config.createLoader(filePath)
        
        extractor = Extractor()
        extractor.load_llm(LLM(self._model))
        
        # Dummy loader cho extractor (c·∫ßn thi·∫øt cho init)
        dummyLoader, _, _ = config.createLoader(filePath)
        extractor.load_document_loader(dummyLoader)
        
        proc = Process()
        proc.load_document_loader(splitLoader)
        proc.add_classify_extractor([[extractor]])
        
        # Ch·ªçn Splitter ph√π h·ª£p (Image ho·∫∑c Text)
        splitter = ImageSplitter(self._model) if vision else TextSplitter(self._model)
        proc.load_splitter(splitter)
        proc.load_file(filePath)
        
        classifications = getClassificationsList()
        for c in classifications:
            c.extractor = extractor
        
        # 2. Th·ª±c hi·ªán t√°ch trang (Split)
        # S·ª≠ d·ª•ng EAGER mode n·∫øu √≠t trang, LAZY mode n·∫øu nhi·ªÅu trang ƒë·ªÉ t·ªëi ∆∞u
        strategy = SplittingStrategy.EAGER if pageCount <= config.processing.eagerPageThreshold else SplittingStrategy.LAZY
        
        try:
            proc.split(classifications, strategy=strategy)
        except KeyError as e:
            # Fallback: N·∫øu ImageSplitter l·ªói (th∆∞·ªùng do file kh√¥ng c√≥ ·∫£nh), th·ª≠ l·∫°i b·∫±ng TextSplitter
            if 'image' in str(e) and vision:
                print("‚ö†Ô∏è Fallback sang TextSplitter do l·ªói x·ª≠ l√Ω ·∫£nh.")
                proc.load_splitter(TextSplitter(self._model))
                proc.split(classifications, strategy=strategy)
            else:
                raise e
        
        groups = proc.doc_groups or []
        print(f"üìä T√¨m th·∫•y {len(groups)} nh√≥m t√†i li·ªáu.")
        
        # 3. S·ª≠a l·ªói ph√¢n trang (Validation & Correction)
        sanitizePageGroups(groups, pageCount)

        # 4. Tr√≠ch xu·∫•t th√¥ng tin (Extract)
        print("üìù ƒêang tr√≠ch xu·∫•t (Process.extract)...")
        
        try:
            results = proc.extract(vision=vision, completion_strategy=self._strategy)
            
            documents = []
            for group, data in zip(groups, results):
                dataDict = data.model_dump() if hasattr(data, 'model_dump') else data
                documents.append({
                    "category": findCategory(group.classification),
                    "docType": group.classification,
                    "data": dataDict,
                    "confidence": getattr(group, 'confidence', None),
                    "_debug": {"loader": loaderName, "vision": vision, "pages": len(group.pages)}
                })
                print(f"   ‚úÖ ƒê√£ tr√≠ch xu·∫•t: {group.classification}")
                
            return {"documents": documents, "error": None}
            
        except Exception as e:
            print(f"‚ùå L·ªói Process.extract: {e}")
            traceback.print_exc()
            return {"documents": [], "error": f"L·ªói x·ª≠ l√Ω: {str(e)}"}

# Alias ƒë·ªÉ t∆∞∆°ng th√≠ch ng∆∞·ª£c n·∫øu c·∫ßn
DocumentAIProcessor = DocumentProcessor
