import os
import nest_asyncio
from typing import Dict, Optional
import traceback

from extract_thinker import (
    Process, Extractor, ImageSplitter, TextSplitter, SplittingStrategy, CompletionStrategy, LLM
)
from core.config import config
from core.classifications import getClassificationsList
from core.utils import (
    countPages, findCategory, 
    makeSuccessResponse, makeErrorResponse
)

nest_asyncio.apply()

class DocumentProcessor:
    """
    B·ªô x·ª≠ l√Ω t√†i li·ªáu trung t√¢m (Document Processor).
    Ch·ªãu tr√°ch nhi·ªám ƒëi·ªÅu ph·ªëi to√†n b·ªô lu·ªìng x·ª≠ l√Ω:     
    T·∫£i file -> Ph√¢n lo·∫°i -> T√°ch trang -> Tr√≠ch xu·∫•t d·ªØ li·ªáu.
    """
    def __init__(self, model: Optional[str] = None, 
                 strategy: CompletionStrategy = CompletionStrategy.CONCATENATE):
        config.validate()
        self._model = model or config.processing.model
        self._strategy = strategy
    
    def run(self, filePath: str) -> Dict:
        """
        Th·ª±c thi lu·ªìng x·ª≠ l√Ω t√†i li·ªáu ch√≠nh.
        
        Quy tr√¨nh:
        1. Ki·ªÉm tra file v√† kh·ªüi t·∫°o Loader (Pypdf ho·∫∑c DocumentAI).
        2. ƒê·∫øm s·ªë trang ƒë·ªÉ quy·∫øt ƒë·ªãnh chi·∫øn l∆∞·ª£c x·ª≠ l√Ω.
        3. C·∫•u h√¨nh Extractor (d√πng chung cho c·∫£ lu·ªìng).
        4. ƒêi·ªÅu h∆∞·ªõng sang x·ª≠ l√Ω ƒê∆°n trang ho·∫∑c ƒêa trang.
        
        Args:
            filePath (str): ƒê∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi t·ªõi file t√†i li·ªáu.
            
        Returns:
            Dict: Dictionary ch·ª©a danh s√°ch t√†i li·ªáu ƒë√£ tr√≠ch xu·∫•t ho·∫∑c th√¥ng b√°o l·ªói.
        """
        if not os.path.exists(filePath):
            return makeErrorResponse("File kh√¥ng t·ªìn t·∫°i")
        
        try:
            # 1. Kh·ªüi t·∫°o Loader v√† ƒë·∫øm s·ªë trang
            loader, vision, loaderName = config.createLoader(filePath)
            pageCount = countPages(filePath)
            
            print(f"üîÑ ƒêang x·ª≠ l√Ω: {loaderName}, vision={vision}, s·ªë trang={pageCount}")
            
            # 2. C·∫•u h√¨nh Extractor
            extractor = Extractor()
            extractor.load_document_loader(loader)
            extractor.load_llm(LLM(self._model))
            
            # 3. Ch·ªçn chi·∫øn l∆∞·ª£c x·ª≠ l√Ω d·ª±a tr√™n s·ªë trang
            if pageCount == 1:
                return self.extractSinglePage(extractor, filePath, vision, loaderName)
            
            return self.extractMultiPage(extractor, filePath, vision, loaderName, pageCount)
                
        except Exception as e:
            traceback.print_exc()
            return makeErrorResponse(str(e)[:200])
    
    def extractSinglePage(self, extractor: Extractor, filePath: str, vision: bool, loaderName: str) -> Dict:
        """
        X·ª≠ l√Ω t√†i li·ªáu ƒê∆°n trang (Single Page).
        
        Chi·∫øn l∆∞·ª£c:
        - Kh√¥ng c·∫ßn t√°ch trang (Split).
        - G·ªçi tr·ª±c ti·∫øp Extractor ƒë·ªÉ Ph√¢n lo·∫°i v√† Tr√≠ch xu·∫•t.
        - T·ªëi ∆∞u hi·ªáu su·∫•t cho file nh·ªè.
        """
        # Ph√¢n lo·∫°i t√†i li·ªáu
        classifications = getClassificationsList()
        result = extractor.classify(filePath, classifications, vision=vision)
        
        if not result or result.name == "Other":
            return makeSuccessResponse(category="Other", loader=loaderName, vision=vision)
        
        contract = result.classification.contract if result.classification else None
        data = None
        
        # Tr√≠ch xu·∫•t d·ªØ li·ªáu n·∫øu t√¨m th·∫•y Contract ph√π h·ª£p
        if contract:
            extractedObj = extractor.extract(filePath, contract, vision=vision, completion_strategy=self._strategy)
            data = extractedObj.model_dump() if hasattr(extractedObj, 'model_dump') else extractedObj
        
        return makeSuccessResponse(
            category=findCategory(result.name) or result.name,
            docType=result.name,
            data=data,
            confidence=getattr(result, 'confidence', None),
            loader=loaderName,
            vision=vision
        )
    
    def extractMultiPage(self, extractor: Extractor, filePath: str, vision: bool, loaderName: str, pageCount: int) -> Dict:
        """
        X·ª≠ l√Ω t√†i li·ªáu ƒêa trang (Multi Page / Mixed Documents).
        
        Chi·∫øn l∆∞·ª£c:
        1. S·ª≠ d·ª•ng Process v√† Splitter ƒë·ªÉ chia nh·ªè file l·ªõn th√†nh c√°c nh√≥m trang (Document Groups).
        2. Ph√¢n lo·∫°i t·ª´ng nh√≥m trang.
        3. Tr√≠ch xu·∫•t d·ªØ li·ªáu cho t·ª´ng nh√≥m.
        
        L∆∞u √Ω:
        - S·ª≠ d·ª•ng l·∫°i Extractor ƒë√£ kh·ªüi t·∫°o ƒë·ªÉ ti·∫øt ki·ªám t√†i nguy√™n.
        - C√≥ c∆° ch·∫ø Fallback t·ª´ ImageSplitter sang TextSplitter n·∫øu c·∫ßn.
        """
        # Bao g·ªìm c√°c b∆∞·ªõc: Split (T√°ch trang) -> Extract (Tr√≠ch xu·∫•t).
        print("üìÑ Ph√°t hi·ªán t√†i li·ªáu nhi·ªÅu trang. ƒêang ti·∫øn h√†nh t√°ch (Splitting)...")
        
        # 1. Chu·∫©n b·ªã Loader ri√™ng bi·ªát cho b∆∞·ªõc Split
        splitLoader, _, _ = config.createLoader(filePath)
        
        # Reuse extractor instance passed from run()
        # No need to create new Extractor or load LLM again
        
        proc = Process()
        proc.load_document_loader(splitLoader)
        proc.add_classify_extractor([[extractor]])
        
        # Ch·ªçn Splitter ph√π h·ª£p (Image ho·∫∑c Text)
        splitter = ImageSplitter(self._model) if vision else TextSplitter(self._model)
        proc.load_splitter(splitter)
        proc.load_file(filePath)
        
        classifications = getClassificationsList()
        for c in classifications:
            c.extractor = extractor
        
        # 2. Th·ª±c hi·ªán t√°ch trang (Split)
        # S·ª≠ d·ª•ng EAGER mode n·∫øu √≠t trang, LAZY mode n·∫øu nhi·ªÅu trang ƒë·ªÉ t·ªëi ∆∞u
        strategy = SplittingStrategy.EAGER if pageCount <= config.processing.eagerPageThreshold else SplittingStrategy.LAZY
        
        try:
            proc.split(classifications, strategy=strategy)
        except KeyError as e:
            # Fallback: N·∫øu ImageSplitter l·ªói (th∆∞·ªùng do file kh√¥ng c√≥ ·∫£nh), th·ª≠ l·∫°i b·∫±ng TextSplitter
            if 'image' in str(e) and vision:
                print("‚ö†Ô∏è Fallback sang TextSplitter do l·ªói x·ª≠ l√Ω ·∫£nh.")
                proc.load_splitter(TextSplitter(self._model))
                proc.split(classifications, strategy=strategy)
            else:
                raise e
        
        groups = proc.doc_groups or []
        print(f"üìä T√¨m th·∫•y {len(groups)} nh√≥m t√†i li·ªáu.")
        
        # 3. Tr√≠ch xu·∫•t th√¥ng tin (Extract)
        print("üìù ƒêang tr√≠ch xu·∫•t (Process.extract)...")
        
        try:
            results = proc.extract(vision=vision, completion_strategy=self._strategy)
            
            documents = []
            
            for group, data in zip(groups, results):
                dataDict = data.model_dump() if hasattr(data, 'model_dump') else data
                
                documents.append({
                    "category": findCategory(group.classification),
                    "docType": group.classification,
                    "data": dataDict,
                    "confidence": getattr(group, 'confidence', None),
                    "_debug": {"loader": loaderName, "vision": vision, "pages": len(group.pages)}
                })
                print(f"   ‚úÖ ƒê√£ tr√≠ch xu·∫•t: {group.classification}")
                
            return {"documents": documents, "error": None}
            
        except Exception as e:
            print(f"‚ùå L·ªói Process.extract: {e}")
            traceback.print_exc()
            return {"documents": [], "error": f"L·ªói x·ª≠ l√Ω: {str(e)}"}

# Alias ƒë·ªÉ t∆∞∆°ng th√≠ch ng∆∞·ª£c n·∫øu c·∫ßn
DocumentAIProcessor = DocumentProcessor
